{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e08812d2",
   "metadata": {},
   "source": [
    "# Langchain Get Started tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b5e4af",
   "metadata": {},
   "source": [
    "LangChain is a framework for developing applications powered by language models\n",
    "\n",
    "- Lang: Stands for language, which is the primary focus of LangChain\n",
    "- Chain:  the connotation of connecting things "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524e9475",
   "metadata": {},
   "source": [
    "## 1. Installation\n",
    "\n",
    "This will install the bare minimum requirements of LangChain. if you are gonna request to opeanAI models, it must be to install openai library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770f8e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc4006b",
   "metadata": {},
   "source": [
    "## 2. Environment setup\n",
    "\n",
    "Depend on your LLM provider, it's necessary to install their Python package, E.g: we're gonna use openAI models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e4f6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7531694",
   "metadata": {},
   "source": [
    "let's create a .env file with credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a386d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8112d9b8",
   "metadata": {},
   "source": [
    "or set within notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95c5a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"<>\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"...\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10be167c",
   "metadata": {},
   "source": [
    "Note: In this example we're are using azureOpenAI services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b467fb0b",
   "metadata": {},
   "source": [
    "## 3. Some concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc82738f",
   "metadata": {},
   "source": [
    "The most common and most important chain that LangChain helps create contains three things:\n",
    "\n",
    "- **LLM:** The language model is the core reasoning engine here \n",
    "\n",
    "- **Prompt Templates:** This provides instructions to the language model. This controls what the language model outputs\n",
    "\n",
    "- **Output Parsers:** These translate the raw response from the LLM to a more workable format, making it easy to use the output downstream.\n",
    "\n",
    "Most LangChain applications allow you to configure the LLM and/or the prompt used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efccf91b",
   "metadata": {},
   "source": [
    "## A) LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4a846f",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TZbfdCyPRLi9ZAORn6WnGw.png\" width=\"500\" height=\"200\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edb7751",
   "metadata": {},
   "source": [
    "There are two types of language models, which in LangChain are called:\n",
    "\n",
    "- **LLMs:** this is a language model which takes a string as input and returns a string\n",
    "    - input: String\n",
    "    - output: String\n",
    "- **ChatModels:** this is a language model which takes a list of messages as input and returns a message\n",
    "    - input: List[ChatMessages]\n",
    "    - output: single ChatMessage\n",
    "\n",
    "\n",
    "**ChatMessage**: has two required components:\n",
    "\n",
    "- **role:** This is the role of the entity from which the ChatMessage is coming from.\n",
    "\n",
    "    - **HumanMessage:** A ChatMessage coming from a human/user.\n",
    "    - **AIMessage:** A ChatMessage coming from an AI/assistant.\n",
    "    - **SystemMessage:** A ChatMessage coming from the system.\n",
    "    - **FunctionMessage:** A ChatMessage coming from a function call.\n",
    "  \n",
    "\n",
    "\n",
    "- **content:** This is the content of the message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a501148",
   "metadata": {},
   "source": [
    "The standard interface that LangChain provides has two methods:\n",
    "\n",
    "- predict: Takes in a string, returns a string\n",
    "- predict_messages: Takes in a list of messages, returns a message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82b5fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.chat_models import AzureChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b089fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureOpenAI(temperature=0.0, deployment_name= \"chatGPTInstruct\", model_name=\"gpt-35-turbo-instruct\", verbose=False)\n",
    "chat_model = AzureChatOpenAI(deployment_name=  \"GPT35-16k\", model_name=\"gpt-35-turbo-16k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41e61a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"What would be a good company name for a company that makes colorful socks?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9035aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.predict(text) # Try to predict next words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62be73c8",
   "metadata": {},
   "source": [
    "![llm completions](../../resources/llm_completion.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c1f7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model.predict(text) # Try to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d7bfb1",
   "metadata": {},
   "source": [
    "Finally, let's use the predict_messages method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101583d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e770fe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "messages = [HumanMessage(content=text)]\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec96f40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.predict_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32da891b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model.predict_messages(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4768c18c",
   "metadata": {},
   "source": [
    "## B) Prompt templates\n",
    "\n",
    "The LLM applications do not pass user input directly into an LLM.\n",
    "\n",
    "**Prompt template:** Provides additional context on the specific task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24c6c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb83cda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt.format(product=\"colorful socks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c8a92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt.format(product=\"food\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc52d45",
   "metadata": {},
   "source": [
    "PromptTemplates can also be used to produce a list of messages\n",
    "\n",
    "The prompt not only contains information about the content, but also each message (its role, its position in the list, etc.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bc4355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7d875c",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "human_template = \"{text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd84bfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([ #List of messages with structure role:..., content\n",
    "    (\"system\", template), # system role\n",
    "    (\"human\", human_template), # human, the user text\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf31568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63436b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = chat_prompt.format_messages(input_language=\"English\",\n",
    "                            output_language=\"Spanish\",\n",
    "                           text= \"Langchain is awesome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5c52d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e37a6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model.predict_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a728f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.predict_messages(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ee5ebd",
   "metadata": {},
   "source": [
    "## C) Output parsers\n",
    "\n",
    "OutputParsers convert the raw output of an LLM into a format that can be used downstream, There are few main types of OutputParsers, including:\n",
    "\n",
    "- Convert text from LLM into structured information (e.g. JSON)\n",
    "- Convert a ChatMessage into just a string\n",
    "- Convert the extra information returned from a call besides the message (like OpenAI function invocation) into a string.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de19363e",
   "metadata": {},
   "source": [
    "Let's write our own output parser - one that converts a comma separated list into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a485477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import BaseOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37f433f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eed4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "CommaSeparatedListOutputParser().parse(\"papa, yuca, platano, maracuya\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41867f1",
   "metadata": {},
   "source": [
    "## Example using: PromptTemplate + LLM + OutputParser [NOT CHAIN, manually]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e04e3f",
   "metadata": {},
   "source": [
    "### 1. Create ChatPromptTemplate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b043a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a helpful assistant who generates comma separated lists.\n",
    "A user will pass in a country, and you should generate 5 food of that country in a comma separated list.\n",
    "ONLY return a comma separated list, and nothing more.\"\"\"\n",
    "\n",
    "human_template = \"{country}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template), \n",
    "    (\"human\", human_template), \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9de9477",
   "metadata": {},
   "source": [
    "### 2. Format ChatPromptTemplate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed41749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = chat_prompt.format_messages(country=\"colombia\")\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e132c60",
   "metadata": {},
   "source": [
    "### 3. Predict messages using ChatPromptTemplate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e941137",
   "metadata": {},
   "outputs": [],
   "source": [
    "food = chat_model.predict_messages(messages).content\n",
    "\n",
    "food"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0a24d9",
   "metadata": {},
   "source": [
    "### 4. Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95216797",
   "metadata": {},
   "outputs": [],
   "source": [
    "CommaSeparatedListOutputParser().parse(food)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a534af",
   "metadata": {},
   "source": [
    "## Example using: PromptTemplate + LLM + OutputParser [CHAIN]\n",
    "\n",
    "just two code lines to do the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc19a725",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_prompt | chat_model | CommaSeparatedListOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930e3424",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"country\":\"Colombia\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6374061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.batch([{\"input\":\"Colombia\"},{\"input\":\"Argentina\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa38c241",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms_lanchain",
   "language": "python",
   "name": "llms_lanchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
