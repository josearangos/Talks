{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a27ed54",
   "metadata": {},
   "source": [
    "# LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f05f79",
   "metadata": {},
   "source": [
    "## What is ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93039b95",
   "metadata": {},
   "source": [
    "LangChain Expression Language or LCEL is a declarative way to easily compose chains together. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531639c2",
   "metadata": {},
   "source": [
    "## Pro\n",
    "\n",
    "There are several benefits to writing our chains using LCEL as opposed to writing normal code\n",
    "\n",
    "1. **Async, Batch, and Streaming Suppor**: you can run your chains automatically full sync, async, batch and streaming without rewriting them.\n",
    "2. **Fallbacks:** The non-determinism behaivor of LLMs makes it important to be able to handle erros easily. With LCEL we can attach fallbacks to any chain. \n",
    "3. **Parallelism:** With LCEL syntax you can run any component in parallel automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05fc567",
   "metadata": {},
   "source": [
    "## 1. Installation\n",
    "\n",
    "This will install the bare minimum requirements of LangChain. if you are gonna request to opeanAI models, it must be to install openai library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7cc11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62cae38",
   "metadata": {},
   "source": [
    "## 2. Environment setup\n",
    "\n",
    "Depend on your LLM provider, it's necessary to install their Python package, E.g: we're gonna use openAI models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa56081",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac89a105",
   "metadata": {},
   "source": [
    "let's create a .env file with credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6ac8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44e90ab",
   "metadata": {},
   "source": [
    "or set within notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c60da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"<>\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"...\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259e040b",
   "metadata": {},
   "source": [
    "Note: In this example we're are using azureOpenAI services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344026bf",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9336fbd",
   "metadata": {},
   "source": [
    "Chains are sequences of instructions the LangChain framework executes to perform a task.\n",
    "\n",
    "To make it as easy as possible to create custom chains, Langchain have implemented a \"Runnable\" protocol.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ffa464",
   "metadata": {},
   "source": [
    "## \"Runnable\" protocol\n",
    "\n",
    "This is a standard interface, which makes it easy to define custom chains as well as invoke them in a standard way. \n",
    "\n",
    "The standard interface includes (ways to run a chain):\n",
    "\n",
    "## Ways to run a chain sync\n",
    "\n",
    "- **stream:** stream back chunks of the response\n",
    "- **invoke:** call the chain on an input\n",
    "- **batch:** call the chain on a list of inputs\n",
    "\n",
    "\n",
    "## Ways to run a chain async\n",
    "\n",
    "These also have corresponding async methods:\n",
    "\n",
    "- **astream:** stream back chunks of the response async\n",
    "- **ainvoke:** call the chain on an input async\n",
    "- **abatch:** call the chain on a list of inputs async\n",
    "- **astream_log:** stream back intermediate steps as they happen, in addition to the final response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a519defe",
   "metadata": {},
   "source": [
    "The **input type** varies by component:\n",
    "\n",
    "| Component | Input Type |\n",
    "| --- | --- |\n",
    "|Prompt|Dictionary|\n",
    "|Retriever|Single string|\n",
    "|LLM, ChatModel| Single string, list of chat messages or a PromptValue|\n",
    "|Tool|Single string, or dictionary, depending on the tool|\n",
    "|OutputParser|The output of an LLM or ChatModel|\n",
    "\n",
    "The **output type** also varies by component:\n",
    "\n",
    "| Component | Output Type |\n",
    "| --- | --- |\n",
    "| LLM | String |\n",
    "| ChatModel | ChatMessage |\n",
    "| Prompt | PromptValue |\n",
    "| Retriever | List of documents |\n",
    "| Tool | Depends on the tool |\n",
    "| OutputParser | Depends on the parser |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c75d61d",
   "metadata": {},
   "source": [
    "All **runnables** expose **input** and **output** schemas to inspect the inputs and outputs:\n",
    "    \n",
    "- **input_schema:** an input Pydantic model auto-generated from the structure of the Runnable\n",
    "- **output_schema:** an output Pydantic model auto-generated from the structure of the Runnable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5282858b",
   "metadata": {},
   "source": [
    "## Chain => (PromptTemplate + ChatModel) \n",
    "\n",
    "Let's create our frist chain using prompt + ChatModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2370ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import AzureChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9118b052",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AzureChatOpenAI(temperature=0.0, deployment_name=  \"GPT35-16k\", model_name=\"gpt-35-turbo-16k\")\n",
    "prompt = ChatPromptTemplate.from_template(\"just tell me the name of most typical course from:{country} and its ingredients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ca47dd",
   "metadata": {},
   "source": [
    "chain = INPUT | MODEL | OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81f7a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d3e3da",
   "metadata": {},
   "source": [
    "## Input Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76e2d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input schema of the chain is the input schema of its first part, the prompt.\n",
    "chain.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd05f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt.input_schema.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e28c0a5",
   "metadata": {},
   "source": [
    "The chain and prompt input are the same, it's logic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dd4edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.input_schema.schema() == prompt.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcc0596",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.input_schema.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cfc574",
   "metadata": {},
   "source": [
    "## Output Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905f0588",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.output_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9f2c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.output_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fff3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.output_schema.schema() == model.output_schema.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591d1c2f",
   "metadata": {},
   "source": [
    "# [ Sync ] Ways to run a chain "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77b1535",
   "metadata": {},
   "source": [
    "## Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7027b3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in chain.stream({\"country\": \"Colombia\"}):\n",
    "    print(s.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0666f891",
   "metadata": {},
   "source": [
    "## Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442503db",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"country\" :\"Colombia\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2252cde",
   "metadata": {},
   "source": [
    "## Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415978d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.batch([{\"country\" :\"Colombia\"},\n",
    "            {\"country\" :\"Argentina\"},\n",
    "            {\"country\" :\"Brasil\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5817d0b",
   "metadata": {},
   "source": [
    "We can set the number of concurrent requests by using the max_concurrency parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f0bb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.batch([{\"country\" :\"Colombia\"},\n",
    "            {\"country\" :\"Argentina\"},\n",
    "            {\"country\" :\"Brasil\"}],\n",
    "           config={\"max_concurrency\": 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae689927",
   "metadata": {},
   "source": [
    "# [ Async ] Ways to run a chain "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd4a3a0",
   "metadata": {},
   "source": [
    "## Async Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4640e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for s in chain.astream({\"country\": \"Colombia\"}):\n",
    "    print(s.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b5584f",
   "metadata": {},
   "source": [
    "## Async Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b91d336",
   "metadata": {},
   "outputs": [],
   "source": [
    "await chain.ainvoke({\"country\":\"Argentina\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f8217d",
   "metadata": {},
   "source": [
    "## Async Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94ac9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "await chain.abatch([{\"country\" :\"Colombia\"},\n",
    "            {\"country\" :\"Argentina\"},\n",
    "            {\"country\" :\"Brasil\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49a47f2",
   "metadata": {},
   "source": [
    "# Parallelism\n",
    "\n",
    "Using a RunnableParallel (often written as a dictionary) it executes each element in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4e59d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b51e6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_name_prompt = ChatPromptTemplate.from_template(\"just tell me the name of most typical course from:{country}\")\n",
    "course_name_chain =  course_name_prompt| model\n",
    "\n",
    "location_prompt = ChatPromptTemplate.from_template(\"tell me the location of :{country}\")\n",
    "location_chain = location_prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0e44ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 1\n",
    "print(course_name_chain.invoke({\"country\":\"Colombia\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d99379",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 1\n",
    "print(location_chain.invoke({\"country\":\"Colombia\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834430c5",
   "metadata": {},
   "source": [
    "Now, Run it parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbaf443",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_chain = RunnableParallel(food=course_name_chain, location=location_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c3e424",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 1\n",
    "print(parallel_chain.invoke({\"country\":\"Colombia\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5907478",
   "metadata": {},
   "source": [
    "## Parallelism on batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18470cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 1\n",
    "print(course_name_chain.batch([{\"country\" :\"Colombia\"},\n",
    "            {\"country\" :\"Argentina\"},\n",
    "            {\"country\" :\"Brasil\"}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b3cd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 1\n",
    "print(location_chain.batch([{\"country\" :\"Colombia\"},\n",
    "            {\"country\" :\"Argentina\"},\n",
    "            {\"country\" :\"Brasil\"}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed8d01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 1\n",
    "print(parallel_chain.batch([{\"country\" :\"Colombia\"},\n",
    "            {\"country\" :\"Argentina\"},\n",
    "            {\"country\" :\"Brasil\"}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92722c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_chain  = prompt_summary | chat_model | parserOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2f7f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_point_chain = prompt_key_point | chat_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903092e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_summ_key = summary_chain | key_point_chain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms_lanchain",
   "language": "python",
   "name": "llms_lanchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
